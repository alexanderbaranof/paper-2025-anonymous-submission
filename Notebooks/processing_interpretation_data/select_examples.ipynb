{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "import pymorphy2\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "PROJECT_PATH = \"/home/alex/paper-2025-anonymous-submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/aspirantura_hse/general_venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/alex/aspirantura_hse/general_venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_json(\n",
    "    os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        \"Data/processed_data/dataset.json\"\n",
    "    ),\n",
    "    orient=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"deepseak_explain\"] = [\"какой то текст для объяснения\" for _ in range(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(subset=[\"annotations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(annot):\n",
    "\n",
    "    searcheable_elements = list()\n",
    "    searcheable_links = list()\n",
    "\n",
    "    if isinstance(annot[\"reference_string\"], list):\n",
    "        for i in range(len(annot[\"reference_string\"])):\n",
    "            searcheable_elements.append(\n",
    "                annot[\"reference_string\"][i]\n",
    "            )\n",
    "    elif isinstance(annot[\"reference_string\"], str) and annot[\"reference_string\"].strip() != \"\":\n",
    "        searcheable_elements.append(\n",
    "                annot[\"reference_string\"]\n",
    "        )\n",
    "\n",
    "    if isinstance(annot[\"reference_url\"], list):\n",
    "        for i in range(len(annot[\"reference_url\"])):\n",
    "            searcheable_links.append(\n",
    "                annot[\"reference_url\"][i]\n",
    "            )\n",
    "    elif isinstance(annot[\"reference_url\"], str) and annot[\"reference_url\"].strip() != \"\":\n",
    "        searcheable_links.append(\n",
    "                annot[\"reference_url\"]\n",
    "        )\n",
    "\n",
    "    return searcheable_elements, searcheable_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_searcheable_elements_and_links(annotations):\n",
    "\n",
    "    searcheable_elements = list()\n",
    "    searcheable_links = list()\n",
    "\n",
    "    for annot in annotations:\n",
    "\n",
    "        if isinstance(annot, dict):\n",
    "\n",
    "            tmp_elements, tmp_links = parse_one_annotation(annot)\n",
    "            searcheable_elements.extend(\n",
    "                tmp_elements\n",
    "            )\n",
    "            searcheable_links.extend(\n",
    "                tmp_links\n",
    "            )\n",
    "\n",
    "        elif isinstance(annot, list):\n",
    "\n",
    "            for single_annot in annot:\n",
    "                tmp_elements, tmp_links = parse_one_annotation(single_annot)\n",
    "                searcheable_elements.extend(\n",
    "                    tmp_elements\n",
    "                )\n",
    "                searcheable_links.extend(\n",
    "                    tmp_links\n",
    "                )\n",
    "    \n",
    "    return searcheable_elements, searcheable_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"annotations_len\"] = dataset[\"annotations\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"searcheable_elements\"] = dataset[\"annotations\"].apply(lambda x: extract_searcheable_elements_and_links(x)[0])\n",
    "dataset[\"searcheable_links\"] = dataset[\"annotations\"].apply(lambda x: extract_searcheable_elements_and_links(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"searcheable_elements_len\"] = dataset[\"searcheable_elements\"].apply(lambda x: len(x))\n",
    "dataset[\"searcheable_links_len\"] = dataset[\"searcheable_links\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.query(\"searcheable_elements_len >= 1 or searcheable_links_len >= 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_link(link):\n",
    "    if \"https://ru.wikipedia.org\" in link:\n",
    "        link = link.split(\"https://ru.wikipedia.org/wiki/\")[1]\n",
    "    elif \"https://ru.wiktionary.org\" in link:\n",
    "        link = link.split(\"https://ru.wiktionary.org/wiki/\")[1]\n",
    "    \n",
    "    if \"#\" in link:\n",
    "        position = link.find(\"#\")\n",
    "        link = link[:position]\n",
    "    \n",
    "    link = re.sub(r'\\(.*?\\)', '', link).strip()\n",
    "\n",
    "    link = re.sub(r'[^a-zA-Zа-яА-ЯёЁ]', ' ', link)\n",
    "    link = re.sub(r'\\s+', ' ', link).strip()\n",
    "\n",
    "    return link.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return \" \".join(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "\n",
    "def lemmatize(tokens: str):\n",
    "    tokens = tokens.split(\" \")\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return \" \".join([morph.parse(token)[0].normal_form for token in tokens])\n",
    "\n",
    "def normalize_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    lemmas = lemmatize(tokens)\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"searcheable_elements\"] = dataset[\"searcheable_elements\"].apply(lambda x: [str.lower(t) for t in x])\n",
    "dataset[\"searcheable_elements_tokenized\"] = dataset[\"searcheable_elements\"].apply(lambda x: [tokenize(t) for t in x])\n",
    "dataset[\"searcheable_elements_normalized\"] = dataset[\"searcheable_elements_tokenized\"].apply(lambda x: [lemmatize(t) for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"searcheable_links\"] = dataset[\"searcheable_links\"].apply(lambda x: [normalize_link(t) for t in x])\n",
    "dataset[\"searcheable_links\"] = dataset[\"searcheable_links\"].apply(lambda x: [str.lower(t) for t in x])\n",
    "dataset[\"searcheable_links_tokenized\"] = dataset[\"searcheable_links\"].apply(lambda x: [tokenize(t) for t in x])\n",
    "dataset[\"searcheable_links_normalized\"] = dataset[\"searcheable_links_tokenized\"].apply(lambda x: [lemmatize(t) for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"searcheable_all\"] = dataset[\"searcheable_elements\"] + dataset[\"searcheable_elements_tokenized\"] + dataset[\"searcheable_elements_normalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_word_play</th>\n",
       "      <th>date</th>\n",
       "      <th>article_url</th>\n",
       "      <th>headline</th>\n",
       "      <th>lead</th>\n",
       "      <th>deepseak_explain</th>\n",
       "      <th>annotations_len</th>\n",
       "      <th>searcheable_elements</th>\n",
       "      <th>searcheable_links</th>\n",
       "      <th>searcheable_elements_len</th>\n",
       "      <th>searcheable_links_len</th>\n",
       "      <th>searcheable_elements_tokenized</th>\n",
       "      <th>searcheable_elements_normalized</th>\n",
       "      <th>searcheable_links_tokenized</th>\n",
       "      <th>searcheable_links_normalized</th>\n",
       "      <th>searcheable_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>[{'headline_substring': 'Дели', 'start_index':...</td>\n",
       "      <td>Саммит лидеров G20 завершился подписанием итог...</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-09-10</td>\n",
       "      <td>https://www.kommersant.ru/doc/6209906</td>\n",
       "      <td>На самом Дели</td>\n",
       "      <td>Что участники саммита G20 предпочли украинской...</td>\n",
       "      <td>какой то текст для объяснения</td>\n",
       "      <td>2</td>\n",
       "      <td>[деле, на са]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[деле, на са]</td>\n",
       "      <td>[дело, на са]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[деле, на са, деле, на са, дело, на са]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            annotations  \\\n",
       "2014  [{'headline_substring': 'Дели', 'start_index':...   \n",
       "\n",
       "                                                summary  is_word_play  \\\n",
       "2014  Саммит лидеров G20 завершился подписанием итог...          True   \n",
       "\n",
       "           date                            article_url       headline  \\\n",
       "2014 2023-09-10  https://www.kommersant.ru/doc/6209906  На самом Дели   \n",
       "\n",
       "                                                   lead  \\\n",
       "2014  Что участники саммита G20 предпочли украинской...   \n",
       "\n",
       "                   deepseak_explain  annotations_len searcheable_elements  \\\n",
       "2014  какой то текст для объяснения                2        [деле, на са]   \n",
       "\n",
       "     searcheable_links  searcheable_elements_len  searcheable_links_len  \\\n",
       "2014                []                         2                      0   \n",
       "\n",
       "     searcheable_elements_tokenized searcheable_elements_normalized  \\\n",
       "2014                  [деле, на са]                   [дело, на са]   \n",
       "\n",
       "     searcheable_links_tokenized searcheable_links_normalized  \\\n",
       "2014                          []                           []   \n",
       "\n",
       "                              searcheable_all  \n",
       "2014  [деле, на са, деле, на са, дело, на са]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.query(\n",
    "    \"article_url == 'https://www.kommersant.ru/doc/6209906'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_search(searcheable_elements, all_texts):\n",
    "    \n",
    "    results = list()\n",
    "\n",
    "    assert isinstance(all_texts, list)\n",
    "    assert len(all_texts) == 3\n",
    "\n",
    "    for elem in searcheable_elements:\n",
    "        for text in all_texts:\n",
    "            results.append(\n",
    "                str.find(text, elem)\n",
    "            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLIAN_COLUMNS = [\"deepseak_explain\"]\n",
    "\n",
    "for explain_column in EXPLIAN_COLUMNS:\n",
    "    dataset[f\"{explain_column}\"] = dataset[f\"{explain_column}\"].apply(lambda x: str.lower(x))\n",
    "    dataset[f\"{explain_column}_tokenized\"] = dataset[f\"{explain_column}\"].apply(lambda x: tokenize(x))\n",
    "    dataset[f\"{explain_column}_normalized\"] = dataset[f\"{explain_column}_tokenized\"].apply(lambda x: lemmatize(x))\n",
    "    dataset[f\"{explain_column}\"] = dataset[f\"{explain_column}\"].apply(lambda x: [x])\n",
    "    dataset[f\"{explain_column}_tokenized\"] = dataset[f\"{explain_column}_tokenized\"].apply(lambda x: [x])\n",
    "    dataset[f\"{explain_column}_normalized\"] = dataset[f\"{explain_column}_normalized\"].apply(lambda x: [x])\n",
    "    dataset[f\"{explain_column}_all\"] = dataset[f\"{explain_column}\"] + dataset[f\"{explain_column}_tokenized\"] + dataset[f\"{explain_column}_normalized\"]\n",
    "\n",
    "    dataset[f\"{explain_column}_search_results\"] = dataset.apply(lambda row: do_search(row[\"searcheable_all\"], row[f\"{explain_column}_all\"]), axis=1)\n",
    "    dataset[f\"{explain_column}_search_results\"] = dataset[f\"{explain_column}_search_results\"].apply(lambda x: max(x))\n",
    "\n",
    "    dataset[f\"{explain_column}_search_results\"] = dataset[f\"{explain_column}_search_results\"].apply(lambda x: True if x >= 0 else False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_word_play</th>\n",
       "      <th>date</th>\n",
       "      <th>article_url</th>\n",
       "      <th>headline</th>\n",
       "      <th>lead</th>\n",
       "      <th>deepseak_explain</th>\n",
       "      <th>annotations_len</th>\n",
       "      <th>searcheable_elements</th>\n",
       "      <th>...</th>\n",
       "      <th>searcheable_links_len</th>\n",
       "      <th>searcheable_elements_tokenized</th>\n",
       "      <th>searcheable_elements_normalized</th>\n",
       "      <th>searcheable_links_tokenized</th>\n",
       "      <th>searcheable_links_normalized</th>\n",
       "      <th>searcheable_all</th>\n",
       "      <th>deepseak_explain_tokenized</th>\n",
       "      <th>deepseak_explain_normalized</th>\n",
       "      <th>deepseak_explain_all</th>\n",
       "      <th>deepseak_explain_search_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>[{'headline_substring': 'Культурная резолюция'...</td>\n",
       "      <td>Белый дом утвердил концепцию развития творческ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-09-28</td>\n",
       "      <td>https://www.kommersant.ru/doc/5007033</td>\n",
       "      <td>Культурная резолюция</td>\n",
       "      <td>В России планируется создать систему поддержки...</td>\n",
       "      <td>[какой то текст для объяснения]</td>\n",
       "      <td>1</td>\n",
       "      <td>[культурная революция]</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[культурная революция]</td>\n",
       "      <td>[культурный революция]</td>\n",
       "      <td>[культурная революция]</td>\n",
       "      <td>[культурный революция]</td>\n",
       "      <td>[культурная революция, культурная революция, к...</td>\n",
       "      <td>[какой то текст для объяснения]</td>\n",
       "      <td>[какой то текст для объяснение]</td>\n",
       "      <td>[какой то текст для объяснения, какой то текст...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>[{'headline_substring': 'Код накликал', 'start...</td>\n",
       "      <td>Зародившееся как антагонист глобальным IТ-лиде...</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>https://www.kommersant.ru/doc/5050986</td>\n",
       "      <td>Код накликал</td>\n",
       "      <td>Зачем и кому нужен Open Source в России</td>\n",
       "      <td>[какой то текст для объяснения]</td>\n",
       "      <td>1</td>\n",
       "      <td>[кот наплакал]</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[кот наплакал]</td>\n",
       "      <td>[кот наплакать]</td>\n",
       "      <td>[кот наплакал]</td>\n",
       "      <td>[кот наплакать]</td>\n",
       "      <td>[кот наплакал, кот наплакал, кот наплакать]</td>\n",
       "      <td>[какой то текст для объяснения]</td>\n",
       "      <td>[какой то текст для объяснение]</td>\n",
       "      <td>[какой то текст для объяснения, какой то текст...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            annotations  \\\n",
       "2077  [{'headline_substring': 'Культурная резолюция'...   \n",
       "2297  [{'headline_substring': 'Код накликал', 'start...   \n",
       "\n",
       "                                                summary  is_word_play  \\\n",
       "2077  Белый дом утвердил концепцию развития творческ...          True   \n",
       "2297  Зародившееся как антагонист глобальным IТ-лиде...          True   \n",
       "\n",
       "           date                            article_url              headline  \\\n",
       "2077 2021-09-28  https://www.kommersant.ru/doc/5007033  Культурная резолюция   \n",
       "2297 2021-10-28  https://www.kommersant.ru/doc/5050986          Код накликал   \n",
       "\n",
       "                                                   lead  \\\n",
       "2077  В России планируется создать систему поддержки...   \n",
       "2297            Зачем и кому нужен Open Source в России   \n",
       "\n",
       "                     deepseak_explain  annotations_len  \\\n",
       "2077  [какой то текст для объяснения]                1   \n",
       "2297  [какой то текст для объяснения]                1   \n",
       "\n",
       "        searcheable_elements  ... searcheable_links_len  \\\n",
       "2077  [культурная революция]  ...                     1   \n",
       "2297          [кот наплакал]  ...                     1   \n",
       "\n",
       "      searcheable_elements_tokenized  searcheable_elements_normalized  \\\n",
       "2077          [культурная революция]           [культурный революция]   \n",
       "2297                  [кот наплакал]                  [кот наплакать]   \n",
       "\n",
       "     searcheable_links_tokenized searcheable_links_normalized  \\\n",
       "2077      [культурная революция]       [культурный революция]   \n",
       "2297              [кот наплакал]              [кот наплакать]   \n",
       "\n",
       "                                        searcheable_all  \\\n",
       "2077  [культурная революция, культурная революция, к...   \n",
       "2297        [кот наплакал, кот наплакал, кот наплакать]   \n",
       "\n",
       "           deepseak_explain_tokenized      deepseak_explain_normalized  \\\n",
       "2077  [какой то текст для объяснения]  [какой то текст для объяснение]   \n",
       "2297  [какой то текст для объяснения]  [какой то текст для объяснение]   \n",
       "\n",
       "                                   deepseak_explain_all  \\\n",
       "2077  [какой то текст для объяснения, какой то текст...   \n",
       "2297  [какой то текст для объяснения, какой то текст...   \n",
       "\n",
       "     deepseak_explain_search_results  \n",
       "2077                           False  \n",
       "2297                           False  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deepseak_explain_search_results    0.000968\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[[f\"{t}_search_results\" for t in EXPLIAN_COLUMNS]].sum() / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
